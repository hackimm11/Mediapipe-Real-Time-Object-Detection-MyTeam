{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning  libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE\n",
      "\n",
      "LibMambaUnsatisfiableError: Encountered problems while solving:\n",
      "  - nothing provides bleach 1.5.0 needed by tensorboard-1.7.0-py35he025d50_1\n",
      "\n",
      "Could not solve for environment specs\n",
      "The following packages are incompatible\n",
      "├─ pin-1 is installable and it requires\n",
      "│  └─ python 3.12.* , which can be installed;\n",
      "└─ tensorflow is not installable because there are no viable options\n",
      "   ├─ tensorflow [1.10.0|1.9.0] would require\n",
      "   │  └─ python 3.5.* , which conflicts with any installable versions previously reported;\n",
      "   ├─ tensorflow [1.10.0|1.11.0|...|2.1.0] would require\n",
      "   │  └─ python 3.6.* , which conflicts with any installable versions previously reported;\n",
      "   ├─ tensorflow [1.13.1|1.14.0|...|2.9.1] would require\n",
      "   │  └─ python 3.7.* , which conflicts with any installable versions previously reported;\n",
      "   ├─ tensorflow [1.7.0|1.7.1|1.8.0] would require\n",
      "   │  └─ tensorboard [>=1.7.0,<1.8.0 |>=1.8.0,<1.9.0 ], which requires\n",
      "   │     └─ bleach 1.5.0 , which does not exist (perhaps a missing channel);\n",
      "   ├─ tensorflow [2.10.0|2.8.2|2.9.1] would require\n",
      "   │  └─ python 3.10.* , which conflicts with any installable versions previously reported;\n",
      "   ├─ tensorflow [2.10.0|2.3.0|...|2.9.1] would require\n",
      "   │  └─ python 3.8.* , which conflicts with any installable versions previously reported;\n",
      "   └─ tensorflow [2.10.0|2.5.0|2.6.0|2.8.2|2.9.1] would require\n",
      "      └─ python 3.9.* , which conflicts with any installable versions previously reported.\n",
      "\n",
      "Pins seem to be involved in the conflict. Currently pinned specs:\n",
      " - python 3.12.* (labeled as 'pin-1')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install mediapipe\n",
    "# %pip install msvc-runtime\n",
    "# !conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là on fait une implementation direct basée sur mediapipe pour la detection d'objets pour une image, et on applique ça dans une boucle pour chaque frame de notre camera stream. Les modeles de détection d'objets contiennet d'ailleurs un classificateurs d'objets, avec une liste un peu limitée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_options = python.BaseOptions(model_asset_path='models/efficientdet_lite0_float16.tflite') #meilleure rapprot\n",
    "# base_options = python.BaseOptions(model_asset_path='models/efficientdet_lite0_int8.tflite')\n",
    "# base_options = python.BaseOptions(model_asset_path='models/efficientdet_lite0_float32.tflite')\n",
    "# base_options = python.BaseOptions(model_asset_path='models/ssd_mobilenet_v2.tflite')\n",
    "options = vision.ObjectDetectorOptions(base_options=base_options, max_results=4, score_threshold=0.5)\n",
    "detector = vision.ObjectDetector.create_from_options(options)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "    detection_result = detector.detect(image)\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    if detection_result.detections:\n",
    "        for detection in detection_result.detections:\n",
    "          \n",
    "            bbox = detection.bounding_box\n",
    "            x, y, w, h = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "\n",
    "            cv2.rectangle(annotated_frame, (x, y), (x + w, y + h), (100, 255, 0), 2)\n",
    "            label = detection.categories[0].category_name if detection.categories else 'Object'\n",
    "            cv2.putText(annotated_frame, label, (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Real-time Object Detection\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Detection(bounding_box=BoundingBox(origin_x=4, origin_y=40, width=630, height=435), categories=[Category(index=None, score=0.8906965255737305, display_name=None, category_name='person')], keypoints=[])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_result.detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LIVE_STREAM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette experimentation on utilise l'option LIVE_STREAM comme running_mode pour le detector_options, qui gère normalement plus fluidement les detection en live, et qui nécissite d'ailleurs un callback ainsi qu'un detecteur asynchrone qui depends des laps de temps des frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run(model: str) -> None:\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        sys.exit(\"ERREUR : Impossible d'ouvrir la webcam.\")\n",
    "\n",
    "    detection_results = []\n",
    "    def result_callback(result: vision.ObjectDetectorResult,\n",
    "                        output_image: mp.Image, timestamp_ms: int):\n",
    "        result.timestamp_ms = timestamp_ms\n",
    "        detection_results.append(result)\n",
    "\n",
    "    base_options = python.BaseOptions(model_asset_path=model)\n",
    "    detector_options = vision.ObjectDetectorOptions(\n",
    "        base_options=base_options,\n",
    "        running_mode=vision.RunningMode.LIVE_STREAM,\n",
    "        score_threshold=0.5,\n",
    "        max_results=5,\n",
    "        result_callback=result_callback\n",
    "    )\n",
    "    detector = vision.ObjectDetector.create_from_options(detector_options)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            sys.exit(\"ERREUR : Impossible de lire depuis la webcam.\")\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "        detector.detect_async(mp_image, int(time.time() * 1000))\n",
    "\n",
    "        output_image = mp_image.numpy_view()\n",
    "        output_image = cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if detection_results:\n",
    "            annotated_image = print_detection(output_image, detection_results[0])\n",
    "            cv2.imshow(\"Object Detector\", annotated_image)\n",
    "            detection_results.clear()\n",
    "        else:\n",
    "            cv2.imshow(\"Object Detector\", output_image)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    detector.close()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\n",
    "    '--model',\n",
    "    help=\"Chemin du modèle (TFLite de détection d'objets)\",\n",
    "    default='models/efficientdet_lite0_int8.tflite')\n",
    "args = parser.parse_args()\n",
    "\n",
    "run(args.model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation d'un classificateur a part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La on utilise directement l'interpréteur TFLite pour la classification. Dans cette approche, le modèle de classification est chargé manuellement, les tenseurs sont gérés explicitement et un fichier de labels est utilisé pour mapper l'indice prédit à une étiquette. Cela offre un contrôle précis sur le prétraitement (redimensionnement, conversion de type, normalisation éventuelle) et sur le post-traitement, mais demande plus de code et peut entraîner des erreurs si l'ordre des labels n'est pas aligné avec les sorties du modèle, ce qui était malheureusement le cas pour moi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "# -------------------------\n",
    "# Chargement du classificateur TFLite\n",
    "# -------------------------\n",
    "\n",
    "def load_labels(label_path: str) -> dict:\n",
    "    labels = {}\n",
    "    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            labels[i] = line.strip()\n",
    "    return labels\n",
    "\n",
    "LABELS = load_labels(\"labels/effiscientnet_lite0_labels_classifier.txt\")\n",
    "classifier_path = \"models/efficientnet_lite0_float32_classifier.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=classifier_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "def classify_crop(image_crop: np.ndarray) -> str:\n",
    "    \n",
    "    input_shape = input_details[0]['shape']  #[1, height, width, channels]\n",
    "    target_height, target_width = input_shape[1], input_shape[2]\n",
    "    resized = cv2.resize(image_crop, (target_width, target_height))\n",
    "    input_data = np.expand_dims(resized, axis=0).astype(np.uint8)\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    pred_index = int(np.argmax(output_data))\n",
    "    return LABELS.get(pred_index, \"inconnu\")\n",
    "\n",
    "# -------------------------\n",
    "# Configuration du détecteur d'objets (MediaPipe Tasks)\n",
    "# -------------------------\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path=\"models/efficientdet_lite0_float16.tflite\")\n",
    "detector_options = vision.ObjectDetectorOptions(\n",
    "    base_options=base_options,\n",
    "    running_mode=vision.RunningMode.IMAGE,  # mode image pour un traitement synchrone\n",
    "    score_threshold=0.5,\n",
    "    max_results=5\n",
    ")\n",
    "detector = vision.ObjectDetector.create_from_options(detector_options)\n",
    "\n",
    "# -------------------------\n",
    "# Capture vidéo et traitement\n",
    "# -------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"Erreur : impossible d'ouvrir la webcam.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "    \n",
    "    detection_result = detector.detect(mp_image)\n",
    "    for detection in detection_result.detections:\n",
    "        bbox = detection.bounding_box\n",
    "        x_min = bbox.origin_x\n",
    "        y_min = bbox.origin_y\n",
    "        x_max = x_min + bbox.width\n",
    "        y_max = y_min + bbox.height\n",
    "\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "        crop = frame[y_min:y_max, x_min:x_max]\n",
    "        if crop.size == 0:\n",
    "            continue  # Sauter si la ROI est invalide\n",
    "        label = classify_crop(crop)\n",
    "        cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Detection & Classification\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois on exploite l'API ImageClassifier de MediaPipe Tasks directement pour la classification. Cette méthode simplifie grandement le pipeline car elle intègre directement dans le modèle (via les métadonnées) l'information sur les étiquettes, renvoyant ainsi le nom de la catégorie et le score sans nécessiter de gestion manuelle des labels. En gros, elle est plus simple, mais génère un peu d'erreurs de prédiction, car deja la façon dont on fait la classification sur le bounding box et le préprocessing peut entrainer des erreurs, en plus, ce classifier contient pluesieurs classes par rapport au classificateur integré de detection d'objets, et elle contient pas exactement les memes classes que l'autre modele aussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- Détection d'objets (EfficientDet par exemple) ---\n",
    "base_options_det = python.BaseOptions(model_asset_path=\"models/efficientdet_lite0_float16.tflite\")\n",
    "detector_options = vision.ObjectDetectorOptions(\n",
    "    base_options=base_options_det,\n",
    "    running_mode=vision.RunningMode.IMAGE,\n",
    "    score_threshold=0.5,\n",
    "    max_results=5\n",
    ")\n",
    "detector = vision.ObjectDetector.create_from_options(detector_options)\n",
    "\n",
    "# --- Classification avec ImageClassifier ---\n",
    "base_options_cls = python.BaseOptions(model_asset_path=\"models/efficientnet_lite0_int8_classifier.tflite\")\n",
    "classifier_options = vision.ImageClassifierOptions(\n",
    "    base_options=base_options_cls,\n",
    "    max_results=4  \n",
    ")\n",
    "classifier = vision.ImageClassifier.create_from_options(classifier_options)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    sys.exit(\"Erreur : impossible d'ouvrir la webcam.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    detection_result = detector.detect(mp_image)\n",
    "    for detection in detection_result.detections:\n",
    "        bbox = detection.bounding_box\n",
    "        x_min = bbox.origin_x\n",
    "        y_min = bbox.origin_y\n",
    "        x_max = x_min + bbox.width\n",
    "        y_max = y_min + bbox.height\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        roi = frame[y_min:y_max, x_min:x_max]\n",
    "        if roi.size == 0:\n",
    "            continue\n",
    "\n",
    "        roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "        mp_roi = mp.Image(image_format=mp.ImageFormat.SRGB, data=roi_rgb)\n",
    "        classification_result = classifier.classify(mp_roi)\n",
    "        top_category = classification_result.classifications[0].categories[0]\n",
    "        label = f\"{top_category.category_name} ({top_category.score:.2f})\"\n",
    "        cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Detection & Classification\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
